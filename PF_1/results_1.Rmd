---
title: "PORTFOLIO 1 - EXERCISE 1"
output:
  html_document:
    df_print: paged
---

## Reading data and libraries
```{r, message = FALSE}
library(tidyverse)
library(mosaic)
library(readxl)

```
```{r}
data_ex1 <- read_excel("data/cities_data.xlsx")
head(data_ex1)
```


# --------------------- Section A -----------------------------

*Make descriptive statistics, e.g. a table with summary statistics (min, mean, std. dev., and max) and a correlation matrix, for the variables that you are going to use and discuss the results (read the whole exercise first so that you know which variables you have to use).*

## Descriptive statistics
```{r}
fav_df <- rbind(
  favstats(data_ex1$Children),
  favstats(data_ex1$Car),
  favstats(data_ex1$HHIncome),
  favstats(data_ex1$Elderly),
  favstats(data_ex1$RoadDens),
  favstats(data_ex1$TransSup),
  favstats(data_ex1$Worker)
)
row.names(fav_df) <- c("Children", "Car", "HHIncome", "Elderly", "RoadDens", "TransSup", "Worker")
fav_df  %>% mutate(across(where(is.numeric), ~ round(., 4)))
```

```{r}
cor(data_ex1)
```
We note variables that are highly correlated with VKMS:

**Children, Car, HHIncome, Elderly, RoadDens, TransSup** and **Worker**



```{r}
aov.out <- aov(VKMS ~ ., data=data_ex1)
summary(aov.out)
```
We note that there is evidence for all variables except for **City** and **CityShape** , having a an effect on the response-variable VKMS, statistically significant at the 0.001 level.

# --------------------- Section B -----------------------------

*Estimate a linear regression explaining the number of vehicle km either in total or per vehicle using relevant socio-economic variables as explanatory variables. Discuss the results.*

First we construct a linear regression model, using all the socio-economic variables that showed correlation with VKMS in the correlation-matrix above, and statistical significance in the ANOVA-test. 

```{r}
data_ex1$Children=as.factor(data_ex1$Children)
model_b <- lm(VKMS ~ Children + Car + HHIncome + Elderly + RoadDens + Worker, data = data_ex1)
summary(model_b)
```
# Irene

```{r}
data_ex1$Children_1 <- ifelse(data_ex1$Children == "1", 1, 0)
data_ex1$Children_2 <- ifelse(data_ex1$Children == "2", 1, 0)
data_ex1$Children_3_or_more <- ifelse(data_ex1$Children == "4", 1, ifelse(data_ex1$Children == "3",1,ifelse(data_ex1$Children == "5",1,0)))
model_b <- lm(VKMS ~ Children_1 + Children_2 + Children_3_or_more + Car + HHIncome + Elderly + RoadDens + Worker, data = data_ex1)
summary(model_b)
```
# Irene


```{r}
model_b_1 <- lm(VKMS ~ Children_1 + Children_2 + Children_3_or_more + Car + HHIncome + RoadDens + Worker, data = data_ex1)
summary(model_b)
```

 $R^2 = 0.5377$ & $F = 664.1$


We see that the Elderly variable has no statistically significant effect on the response-variable, so we decide to remove this variable.

```{r}
model_b <- lm(VKMS ~ Children_1 + Children_2 + Children_3_or_more + Car + HHIncome + RoadDens + Worker, data = data_ex1)
summary(model_b)
```
 $R^2 = 0.5378$ and $F = 759.1$

The result is practically no change in $R^2$. Adding variables can only increase the $R^2$ or have it remain unchanged. Usually, when the $R^2$ is identical, we prefer the simpler model. This "simplicity" or scarcity of variables is reflected in the F-score, in our case we see that the F-score has increased, by dropping the Elderly variable, indicating that this variable was not doing much to explain the variations of our response-variable VKMS.


We can also try to change our predictor variable into annual vehicle kilometers(VKMS) per car, by divding the two, and then constructing a model that predicts this new composite variable.
```{r}
km_per_car_model <- lm(VKMS/Car ~ Children_1 + Children_2 + Children_3_or_more + HHIncome + RoadDens + Worker, data = data_ex1)
summary(km_per_car_model)
```
$R^2 = 0.4409$ and $F = 900$

Both model metrics have been significantly reduced. Number of cars seems to be more useful to our purpose as a predictor, than as a composite of the response.

 
We also have logarithmic transformations of the variables. We can try to use the natural logarithm of VKMS as our response variable, and we can include the natural logarithm of income as a predictor.

First we construct a model including all variables

```{r}
model_ln_b <- lm(LN_VKMS ~ Children_1 + Children_2 + Children_3_or_more + Car + HHIncome + LN_INC + Elderly + RoadDens + Worker, data = data_ex1)
summary(model_ln_b)
```
 $R^2 = 0.6008$ and $F = 763.9$
 
The R-squared is immediately higher than the purely linear models.
 
Now we would like to see if we can simplify the model, without loosing its power. First, we remove the linear *HHIncome* variable, because it is so highly correlated with *LN_INC*

```{r}
model_ln_b <- lm(LN_VKMS ~ Children_1 + Children_2 + Children_3_or_more + Car + LN_INC + RoadDens + Worker + Elderly, data = data_ex1)
summary(model_ln_b)
```
 $R^2 = 0.6002$ and $F = 856.8$
 
The model has lost a negligible amount of $R^2$, but the F-score has improved. It seems that the logarithmic income variable is sufficient in explaining the variation
 
We see also that two variables are not statistically significant even below the 0.1 level. If we look at the correlation-matrix, we see that *Elderly* are highly negatively correlated with *Worker*. Which makes sense, logically speaking, as some degree of elders may be retired. We choose to keep the *Worker* variable, as it covers a wider range of people.

```{r}
model_ln_b <- lm(LN_VKMS ~ Children_1 + Children_2 + Children_3_or_more + Car + HHIncome + LN_INC + RoadDens + Worker, data = data_ex1)
summary(model_ln_b)
```
 $R^2 = 0.6008$ and $F = 859.2$
 
$R^2$ remains practically unchanged from removing the elderly variable, but the F-score has improved. Now we can assign additional levels of variation to be explained by the *Worker* variable, because we have removed the colinearity effect of the *Worker* and *Elderly* variables, explaining the same variation. *Worker* is sufficiently explaining the variation that we had assigned to *Elderly* in the previous model.

# --------------------- Section C -----------------------------
*Add the supply and city variables to the model and redo the estimation. Discuss the results.*

Now we add the TransSup and city variables

```{r}
model_c <- lm(VKMS ~ Children_1 + Children_2 + Children_3_or_more + Car + HHIncome + RoadDens + TransSup + Worker + City + CityShape, data = data_ex1)
summary(model_c)
```
$R^2 = 0.5794$ and $F = 629.4$

Immediately we see an increase in $R^2$, pointing towards some of the variables being able to explain additional sources of variations.
We note that the City variable seems not to be a statistically significant predictor for the response-variable VKMS. If we try to remove it:

```{r}
model_c <- lm(VKMS ~ Children_1 + Children_2 + Children_3_or_more + Car + HHIncome + RoadDens + TransSup + Worker + CityShape, data = data_ex1)
summary(model_c)
```
$R^2 = 0.5795$ and $F = 699.3$

The $R^2$ value is unchanged, while the F-score increases. Using the same rationale as previously, we would prefer the simpler model.

And again, we can also infer the logarithmic model

```{r}
model_ln_c <- lm(LN_VKMS ~ LN_INC + Elderly + Children_1 + Children_2 + Children_3_or_more + Car + HHIncome + RoadDens + TransSup + Worker + CityShape, data=data_ex1)
summary(model_ln_c)
```
$R^2 = 0.6499$ and $F = 770.7$


Again, we remove the variables that can eliminate the main colinearity effects, *Elderly* and *HHIncome*

```{r}
model_ln_c <- lm(LN_VKMS ~ Car + RoadDens + Children_1 + Children_2 + Children_3_or_more + Worker + LN_INC + TransSup + CityShape, data=data_ex1)
summary(model_ln_c)
```
$R^2 = 0.6494$ and $F = 939.5$

Going purely from $R^2$ and F-score, this is our best model so far


# --------------------- Section D -----------------------------
*Compare the model from b) with the model from c). What model do you prefer and why?*

We can run an analysis of variance between the two models:

```{r}
anova(model_b, model_c)
```

Thanks to this comparative analysis of variance between both models, we can conclude that Model_c is better than Model_b:
1) Model_c is significantly different from Model_b, since pvalue is <0.05
2) Model_c' RSS is lower than Model_b's. This means that Model_c manages to decrease the unexplained variance. 

And again, we can run the same analysis between the logarithmic models:

```{r}
anova(model_ln_b, model_ln_c)
```

Thanks to this comparative analysis of variance between both models, we can conclude that Model_ln_c is better than Model_ln_b:
1) Model_ln_c is significantly different from Model_ln_b, since pvalue is <0.05
2) Model_ln_c's RSS is quite lower than Model_ln_b's. 
This means that Model_ln_c manages to decrease the unexplained variance. 

### F TEST
```{r}
var.test(model_b, model_c, alternative = "two.sided")
```


*H0: Variances are equal -> Ratio of variances is 1*
*H1: Variances are unequal -> Ratio of variances is different from 1*
Results: true ratio of variances is not equal to 1, p-value = 0.001901 (95%)

### F TEST
```{r}
var.test(model_ln_b, model_ln_c, alternative = "two.sided")
```


*H0: Variances are equal -> Ratio of variances is 1*
*H1: Variances are unequal -> Ratio of variances is different from 1*
Results: true ratio of variances is not equal to 1, p-value < 2.2e-16 (95%)

OUR BEST MODEL IS model_ln_c
# --------------------- Section E -----------------------------
*Calculate the average elasticity of VKMS with respect to household income and mass transit supply. Interpret the elasticity that you find.*

First, we look at the linear model, and calculate the elasticity for **Household Income**:


```{r}
data_ex1$prediction_c <- predict(model_c, data_ex1)
data_ex1$elasticity_HHIncome_c = model_c$coefficients["HHIncome"]*(data_ex1$HHIncome/data_ex1$prediction_c)
head(select(data_ex1, c("VKMS", "prediction_c", "HHIncome", "elasticity_HHIncome_c")))
```
```{r}
mean(data_ex1$elasticity_HHIncome_c)
```
Result: 0.54
Interpretation: it represents the percentage change expected by a change in the income. 
If the average elasticity is 0.54, then a change of 10% in HH_income leads to an expected change of $0.54*10\%=5.4\%$  in the number of kms 


Then we look at the elasticity for **Transit supply**:

```{r}
data_ex1$elasticity_TransSup_c = model_c$coefficients["TransSup"]*(data_ex1$TransSup/data_ex1$prediction_c)
head(select(data_ex1, c("VKMS", "prediction_c", "TransSup", "elasticity_TransSup_c")))
```

```{r}
mean(data_ex1$elasticity_TransSup_c)
```
Result: -1.19
Interpretation: it represents the percentage change expected by a change in the income. 
If the average elasticity is 0.78, then a change of 10% in Transit supply leads to an expected change of $-1.19*10\% = -11.9\% $  in the number of kms 



As we have established earlier, the logarithmic model does a better job of predicting the *annual vehicle kilometers per household*. So we are also interested in the elasticities for this model

```{r}
data_ex1$prediction_ln_c <- predict(model_ln_c, data_ex1)
data_ex1$ln_elasticity_LN_INC_c = model_ln_c$coefficients["LN_INC"]
head(select(data_ex1, c("LN_VKMS", "prediction_ln_c", "LN_INC", "ln_elasticity_LN_INC_c")))
```


```{r}
data_ex1$ln_elasticity_TransSup_c = model_ln_c$coefficients["TransSup"]*(data_ex1$TransSup/data_ex1$prediction_ln_c)
head(select(data_ex1, c("LN_VKMS", "prediction_ln_c", "TransSup", "ln_elasticity_TransSup_c")))
```
```{r}
mean(data_ex1$ln_elasticity_TransSup_c)
```


# --------------------- Section F -----------------------------
*Calculate the effect on the driving across the cities in a future scenario where all cities are expected to increase their mass transit supply with 50% while income is expected to increase by 10% (all remaining variables are assumed to be unchanged).*


We make our predictions using our elasticities calculated. First, for the linear model:

```{r}
data_ex1$prediction_new_elasticity = 
    data_ex1$prediction_c + data_ex1$HHIncome*0.10*mean(data_ex1$elasticity_HHIncome_c) +
    data_ex1$TransSup*0.50*mean(data_ex1$elasticity_TransSup_c)
head(select(data_ex1,c("prediction_new_elasticity","prediction_c")))
```

```{r}
mean(data_ex1$prediction_new_elasticity - data_ex1$prediction_c)
```
On average, the linear model predicts a 33 km increase in annual vehicle kilometers per household in this scenario.

Let us look at what the logarithmic model predicts:

```{r}
data_ex1$ln_prediction_new_elasticity = 
  data_ex1$prediction_ln_c + log(data_ex1$HHIncome*0.10)*mean(data_ex1$ln_elasticity_LN_INC_c) +
  data_ex1$TransSup*0.50*mean(data_ex1$ln_elasticity_TransSup_c)
head(select(data_ex1,c("ln_prediction_new_elasticity","prediction_ln_c")))
```
We have to remove some observations where the household income was reported as 0, since $log(0) = -\infty$

```{r}
data_ex1 %>% 
  arrange(ln_prediction_new_elasticity) %>%
  select(ln_prediction_new_elasticity) %>%
  head()
```


```{r}
data_temp <- data_ex1 %>% 
  filter(ln_prediction_new_elasticity > 0)
mean(data_temp$ln_prediction_new_elasticity - data_temp$prediction_ln_c)
```
The model predicts an average increase of $log(1.776081)$ in this scenario. If we take the exponential of this number:

```{r}
exp(1.776081)
```


So the average increase in this scenario is 5.9 annual vehicle kilometers.

# --------------------- Section G -----------------------------

*Assuming your model to be correct, briefly discuss uncertainties in your forecast in f).*

# Irene


