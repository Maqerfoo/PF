---
title: "PORTFOLIO 1 - EXERCISE 1"
output:
  html_document:
    df_print: paged
---

## Reading data and libraries
```{r, message = FALSE}
library(tidyverse)
library(mosaic)
library(readxl)

```
```{r}
data_ex1 <- read_excel("data/cities_data.xlsx")
head(data_ex1)
```

## Converting explanatory variables into factors
```{r}
#data_ex1$Children=as.factor(data_ex1$Children)
#data_ex1$Elderly=as.factor(data_ex1$Elderly)
#data_ex1$Worker=as.factor(data_ex1$Worker)
```


# --------------------- Section A -----------------------------

*Make descriptive statistics, e.g. a table with summary statistics (min, mean, std. dev., and max) and a correlation matrix, for the variables that you are going to use and discuss the results (read the whole exercise first so that you know which variables you have to use).*

## Descriptive statistics
```{r}
fav_df <- rbind(
  favstats(data_ex1$Children),
  favstats(data_ex1$Car),
  favstats(data_ex1$HHIncome),
  favstats(data_ex1$Elderly),
  favstats(data_ex1$RoadDens),
  favstats(data_ex1$TransSup),
  favstats(data_ex1$Worker)
)
row.names(fav_df) <- c("Children", "Car", "HHIncome", "Elderly", "RoadDens", "TransSup", "Worker")
fav_df  %>% mutate(across(where(is.numeric), ~ round(., 4)))
```

```{r}
select(data_ex1, -c("LN_INC", "LN_VKMS"))  %>% cor()
```
We note variables that are highly correlated with VKMS:

**Children, Car, HHIncome, Elderly, RoadDens, TransSup** and **Worker**

```{r}
aov.out <- aov(VKMS ~ ., data=data_ex1)
summary(aov.out)
```
We note that there is evidence for all variables except for **City** and **CityShape** , having a an effect on the response-variable VKMS, statistically significant at the 0.001 level.

# --------------------- Section B -----------------------------

*Estimate a linear regression explaining the number of vehicle km either in total or per vehicle using relevant socio-economic variables as explanatory variables. Discuss the results.*

First we construct a linear regression model, using all the socio-economic variables that showed correlation with VKMS in the correlation-matrix above, and statistical significance in the ANOVA-test. 

```{r}
model_b <- lm(VKMS ~ Children + Car + HHIncome + Elderly + RoadDens + Worker, data = data_ex1)
summary(model_b)
```

 $R^2 = 0.5366$ & $F = 881.2$


We see that the Elderly variable has no statistically significant effect on the response-variable, so we decide to remove this variable.

```{r}
model_b <- lm(VKMS ~ Children + Car + HHIncome + RoadDens + Worker, data = data_ex1)
summary(model_b)
```
 $R^2 = 0.5367$ and $F = 1058$

The result is practically no change in $R^2$. Adding variables can only increase the $R^2$ or have it remain unchanged. Usually, when the $R^2$ is identical, we prefer the simpler model. This "simplicity" or scarcity of variables is reflected in the F-score, in our case we see that the F-score has increased, by dropping the Elderly variable, indicating that this variable was not doing much to explain the variations of our response-variable VKMS.


We can also try to change our predictor variable into annual vehicle kilometers(VKMS) per car, by divding the two, and then constructing a model that predicts this new composite variable.
```{r}
km_per_car_model <- lm(VKMS/Car ~ Children + HHIncome + RoadDens + Worker, data = data_ex1)
summary(km_per_car_model)
```
$R^2 = 0.4409$ and $F = 900$

Both model metrics have been significantly reduced. Number of cars seems to be more useful to our purpose as a predictor, than as a composite of the response.

$\color{red}{\text{If I look at the output of the models below, I don't see the model with the children dummy variables being better?}}$

Since results are similar in 3,4 and 5 childrens, then we create dummies variables.
There should be a reference level


```{r}
data_ex1$Children_1 <- ifelse(data_ex1$Children == "1", 1, 0)
data_ex1$Children_2 <- ifelse(data_ex1$Children == "2", 1, 0)
data_ex1$Children_3_or_more <- ifelse(data_ex1$Children == "4", 1, ifelse(data_ex1$Children == "3",1,ifelse(data_ex1$Children == "5",1,0)))
```


```{r}
model_b_dummy_children=lm(VKMS~ Car + HHIncome + RoadDens  + Children_1 + Children_2 + Children_3_or_more + Worker,data = data_ex1)
summary(model_b_dummy_children)
```
 $R^2 = 0.5378$ and $F = 759.1$
 
We also have logarithmic transformations of the variables. We can try to use the natural logarithm of VKMS as our response variable, and we can include the natural logarithm of income as a predictor.

First we construct a model including all variables

```{r}
model_ln_b <- lm(LN_VKMS ~ Children + Car + HHIncome + LN_INC + Elderly + RoadDens + Worker, data = data_ex1)
summary(model_ln_b)
```
 $R^2 = 0.6$ and $F = 978.5$
 
 This model is better at explaining the variation than previous models. We see again that Elderly is the least significant variable, so we try to remove it.

```{r}
model_ln_b <- lm(LN_VKMS ~ Children + Car + HHIncome + LN_INC + RoadDens + Worker, data = data_ex1)
summary(model_ln_b)
```
 $R^2 = 0.6$ and $F = 1141$
 
$R^2$ remains unchanged from removing the elderly variable. In our model, whether or not the household has any elderly people was not explaining any of the variation, so the $R^2$ value remains unchanged, while the F-score increases. Generally speaking, we would prefer this model over the other one.

# --------------------- Section C -----------------------------
*Add the supply and city variables to the model and redo the estimation. Discuss the results.*

Now we add the TransSup and city variables

```{r}
model_c <- lm(VKMS ~ Children + Car + HHIncome + RoadDens + TransSup + Worker + City + CityShape, data = data_ex1)
summary(model_c)
```
$R^2 = 0.5782$ and $F = 782.5$

Immediately we see an increase in $R^2$, pointing towards some of the variables being able to explain additional sources of variations.
We note that the City variable seems not to be a statistically significant predictor for the response-variable VKMS. If we try to remove it:

```{r}
model_c <- lm(VKMS ~ Children + Car + HHIncome + RoadDens + TransSup + Worker + CityShape, data = data_ex1)
summary(model_c)
```
$R^2 = 0.5782$ and $F = 894.3$

The $R^2$ value is unchanged, while the F-score increases. Using the same rationale as previously, we would prefer the simpler model.

And again, we can also include the logarithmic transformed variables

```{r}
model_ln_c <- lm(LN_VKMS ~ ., data=data_ex1)
summary(model_ln_c)
```
The $R^2$ and F-score here is deceptive, as we have VKMS included as an explanatory variable for LN_VKMS.

So, we remove the variables that shows no statistically significant effect above the 0.05-level on the response-variable.

```{r}
model_ln_c <- lm(LN_VKMS ~ Car + HHIncome + RoadDens + Children + Worker + LN_INC + TransSup + CityShape, data=data_ex1)
summary(model_ln_c)
```
$R^2 = 0.6489$ and $F = 1055$

Going purely from $R^2$ and F-score, this is our best model so far


# --------------------- Section D -----------------------------
*Compare the model from b) with the model from c). What model do you prefer and why?*

We can run an analysis of variance between the two models:

```{r}
anova(model_b, model_c)
```

Thanks to this comparative analysis of variance between both models, we can conclude that Model_c is better than Model_b:
1) Model_c is significantly different from Model_b, since pvalue is <0.05
2) Model_c' RSS is lower than Model_b's. This means that Model_c manages to decrease the unexplained variance. 

And again, we can run the same analysis between the logarithmic models:

```{r}
anova(model_ln_b, model_ln_c)
```

Thanks to this comparative analysis of variance between both models, we can conclude that Model_ln_c is better than Model_ln_b:
1) Model_ln_c is significantly different from Model_ln_b, since pvalue is <0.05
2) Model_ln_c's RSS is quite lower than Model_ln_b's. 
This means that Model_ln_c manages to decrease the unexplained variance. 

### F TEST
```{r}
var.test(model_b, model_c, alternative = "two.sided")
```


*H0: Variances are equal -> Ratio of variances is 1*
*H1: Variances are unequal -> Ratio of variances is different from 1*
Results: true ratio of variances is not equal to 1, p-value = 0.001901 (95%)

### F TEST
```{r}
var.test(model_ln_b, model_ln_c, alternative = "two.sided")
```


*H0: Variances are equal -> Ratio of variances is 1*
*H1: Variances are unequal -> Ratio of variances is different from 1*
Results: true ratio of variances is not equal to 1, p-value < 2.2e-16 (95%)

OUR BEST MODEL IS Model_ln_c
# --------------------- Section E -----------------------------
*Calculate the average elasticity of VKMS with respect to household income and mass transit supply. Interpret the elasticity that you find.*

$\color{red}{\text{Here I am getting some different elasticities from yours, in both models}}$

```{r}
Elasticity_HHIncome= 6.6552*data_ex1$HHIncome/(-9562.1497 + 5406.8619*data_ex1$Car + 6.6552*data_ex1$HHIncome + 24083.2327*data_ex1$RoadDens + 1466.2595*data_ex1$   Children_1 + 5006.1883*data_ex1$   Children_2 + 6394.1939*data_ex1$   Children_3_or_more -120.1813*data_ex1$Elderly + 3296.5094*data_ex1$Worker - 5410.8987*data_ex1$TransSup)
(Elasticity_HHIncome_av= mean(Elasticity_HHIncome)) 
```
First, we look at the linear model, and calculate the elasticity for **Household Income**:


```{r}
data_ex1$prediction_c <- predict(model_c, data_ex1)
data_ex1$elasticity_HHIncome_c = model_c$coefficients["HHIncome"]*(data_ex1$HHIncome/data_ex1$prediction_c)
head(select(data_ex1, c("VKMS", "prediction_c", "HHIncome", "elasticity_HHIncome_c")))
```
```{r}
mean(data_ex1$elasticity_HHIncome_c)
```
Result: 0.41
Interpretation: it represents the percentage change expected by a change in the income. 
If the average elasticity is 0.41, then a change of 10% in HH_income leads to an expected change of $0.41*10\%=4.1\%$  in the number of kms 


Then we look at the elasticity for **Transit supply**:

```{r}
data_ex1$elasticity_TransSup_c = model_c$coefficients["TransSup"]*(data_ex1$TransSup/data_ex1$prediction_c)
head(select(data_ex1, c("VKMS", "prediction_c", "TransSup", "elasticity_TransSup_c")))
```

```{r}
mean(data_ex1$elasticity_TransSup_c)
```
Result: -0.78
Interpretation: it represents the percentage change expected by a change in the income. 
If the average elasticity is 0.78, then a change of 10% in Transit supply leads to an expected change of $-0.78*10\% = -7.8\% $  in the number of kms 



```{r}
Elasticity_TransSup= - 5410.8987*data_ex1$TransSup/(-9562.1497 + 5406.8619*data_ex1$Car + 6.6552*data_ex1$HHIncome + 24083.2327*data_ex1$RoadDens + 1466.2595*data_ex1$   Children_1 + 5006.1883*data_ex1$   Children_2 + 6394.1939*data_ex1$   Children_3_or_more -120.1813*data_ex1$Elderly + 3296.5094*data_ex1$Worker - 5410.8987*data_ex1$TransSup)
(Elasticity_TransSup_av= mean(Elasticity_TransSup))
```
As we have established earlier, the linear model does a better job of predicting the *annual vehicle kilometers per household*.

```{r}
data_ex1$prediction_ln_c <- predict(model_ln_c, data_ex1)
data_ex1$ln_elasticity_LN_INC_c = (model_ln_c$coefficients["LN_INC"]*1/data_ex1$prediction_ln_c)*(data_ex1$LN_INC/data_ex1$prediction_ln_c)
data_ex1$ln_elasticity_HHIncome_c = model_ln_c$coefficients["HHIncome"]*(data_ex1$HHIncome/data_ex1$prediction_ln_c)
head(select(data_ex1, c("LN_VKMS", "prediction_ln_c", "LN_INC", "ln_elasticity_LN_INC_c", "ln_elasticity_HHIncome_c")))
```

```{r}
mean(data_ex1$ln_elasticity_HHIncome_c)
mean(data_ex1$ln_elasticity_LN_INC_c)
```

```{r}
data_ex1$ln_elasticity_TransSup_c = model_ln_c$coefficients["TransSup"]*(data_ex1$TransSup/data_ex1$prediction_ln_c)
head(select(data_ex1, c("LN_VKMS", "prediction_ln_c", "TransSup", "ln_elasticity_TransSup_c")))
```
```{r}
mean(data_ex1$ln_elasticity_TransSup_c)
```



```{r}
# Average elasticity of mass transit supply
 # -0.54705
# Result: -0.54705
# Interpretation: it represents the percentage change expected by a change in the supply of mass transit 
# If the average elasticity is -0.547, then a change of 10% in TransSup leads to an expected change of -0.547*10=5.47% in the number of kms 
```



```{r}
# ---- WITH LN
# Average elasticity of household income
Elasticity_LN_INC= 0.44991*data_ex1$LN_INC/(5.38803 + 0.31517*data_ex1$Car + 0.44991*data_ex1$LN_INC + 1.56023*data_ex1$RoadDens + 0.08066*data_ex1$   Children_1 + 0.28958*data_ex1$   Children_2 + 0.38505*data_ex1$   Children_3_or_more -0.03120*data_ex1$Elderly + 0.03656*data_ex1$Worker - 0.37800*data_ex1$TransSup)
(Elasticity_LN_INC_av= mean(Elasticity_LN_INC)) 
# Result: 0.3373478
# Interpretation: it represents the percentage change expected by a change in the income. 
# If the average elasticity is 0.337, then a change of 10% in HH_income leads to an expected change of 0.337*10=3.37% in the number of kms 
```

# --------------------- Section F -----------------------------
*Calculate the effect on the driving across the cities in a future scenario where all cities are expected to increase their mass transit supply with 50% while income is expected to
increase by 10% (all remaining variables are assumed to be unchanged).*


First we make predictions using our elasticities calculated for the linear model

```{r}
data_ex1$prediction_new_elasticity = 
    data_ex1$prediction_c + data_ex1$HHIncome*0.10*mean(data_ex1$elasticity_HHIncome_c) +
    data_ex1$TransSup*0.50*mean(data_ex1$elasticity_TransSup_c)
head(select(data_ex1,c("prediction_new_elasticity","prediction_c")))
```

```{r}
mean(abs(data_ex1$prediction_new_elasticity - data_ex1$prediction_c))
```
On average, the linear model predicts a 25 km increase in annual vehicle kilometers per household in this scenario.

Let us look at what the logarithmic model predicts:

```{r}
data_ex1$ln_prediction_new_elasticity = 
  data_ex1$prediction_ln_c + log(data_ex1$HHIncome*0.10)*mean(data_ex1$ln_elasticity_LN_INC_c) +
  data_ex1$HHIncome*0.10*mean(data_ex1$ln_elasticity_HHIncome_c) +
  data_ex1$TransSup*0.50*mean(data_ex1$ln_elasticity_TransSup_c)
head(select(data_ex1,c("ln_prediction_new_elasticity","prediction_ln_c")))
```
We have to remove some observations where the household income was reported as 0, since $log(0) = -\infty$

```{r}
data_ex1 %>% 
  arrange(ln_prediction_new_elasticity) %>%
  select(ln_prediction_new_elasticity) %>%
  head()
```


```{r}
data_temp <- data_ex1 %>% 
  filter(ln_prediction_new_elasticity > 0)
mean(data_temp$ln_prediction_new_elasticity - data_temp$prediction_ln_c)
```
So the average change in the log(annual vehicle kilometers) is predicted by the logarithmic model to be -0.22 in this scenario. The sign has actually changed.


Finally, we want to see how the results look, if we change the underlying variables, instead of using our elasticity.

```{r}
# Old situation
#Model_c_2_pred = predict(Model_c_2, newdata = data_ex1, interval="prediction")
# Model_c_2_conf = predict(Model_c_2,data_ex1,interval="confidence")
#(Model_c_2_pred_av= mean(Model_c_2_pred)) # Result: the average number in km would be 17701.43
```

```{r}
# New situation
#data_ex1_2<-data_ex1
#data_ex1_2$HHIncome=data_ex1_2$HHIncome*1.1
#data_ex1_2$TransSup=data_ex1_2$TransSup*1.5
#Model_f_pred = predict(Model_c_2,newdata = data_ex1_2, interval="prediction")
# Model_f_conf = predict(Model_c_2,newdata = data_ex1_2,interval="confidence")
#(Model_f_pred_av= mean(Model_f_pred)) # Result: the average number in km would be 17701.43
# QUESTION Results are not correct. They lead to the same value, but it has to be different.
```

 


# --------------------- Section F -----------------------------

Model_f_conf = predict(Model_c_2,newdata = data_ex1_2,interval="confidence")

# Discuss uncertainties in your forecast in f).-> output
# Because they depend on different things: structure, input, parameters

# Model structure (how the model represents the system object):

# Model input (data collected): the model for prediction has been obtained from a specific
# data, so there is uncertainity in the data collected. 
#** They meet the hipothesis of normality, linearity, ... atipical points

# Model parameters (parameters calibrated based on sample)


